{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "JustinNichols_6c_Natural_Language_Processing.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNnDwdyj_JXl"
      },
      "source": [
        "## CSCI 470 Activities and Case Studies\n",
        "\n",
        "1. For all activities, you are allowed to collaborate with a partner. \n",
        "1. For case studies, you should work individually and are **not** allowed to collaborate.\n",
        "\n",
        "By filling out this notebook and submitting it, you acknowledge that you are aware of the above policies and are agreeing to comply with them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuXdTDdl_JXn"
      },
      "source": [
        "Some considerations with regard to how these notebooks will be graded:\n",
        "\n",
        "1. You can add more notebook cells or edit existing notebook cells other than \"# YOUR CODE HERE\" to test out or debug your code. We actually highly recommend you do so to gain a better understanding of what is happening. However, during grading, **these changes are ignored**. \n",
        "2. You must ensure that all your code for the particular task is available in the cells that say \"# YOUR CODE HERE\"\n",
        "3. Every cell that says \"# YOUR CODE HERE\" is followed by a \"raise NotImplementedError\". You need to remove that line. During grading, if an error occurs then you will not receive points for your work in that section.\n",
        "4. If your code passes the \"assert\" statements, then no output will result. If your code fails the \"assert\" statements, you will get an \"AssertionError\". Getting an assertion error means you will not receive points for that particular task.\n",
        "5. If you edit the \"assert\" statements to make your code pass, they will still fail when they are graded since the \"assert\" statements will revert to the original. Make sure you don't edit the assert statements.\n",
        "6. We may sometimes have \"hidden\" tests for grading. This means that passing the visible \"assert\" statements is not sufficient. The \"assert\" statements are there as a guide but you need to make sure you understand what you're required to do and ensure that you are doing it correctly. Passing the visible tests is necessary but not sufficient to get the grade for that cell.\n",
        "7. When you are asked to define a function, make sure you **don't** use any variables outside of the parameters passed to the function. You can think of the parameters being passed to the function as a hint. Make sure you're using all of those variables.\n",
        "8. Finally, **make sure you run \"Kernel > Restart and Run All\"** and pass all the asserts before submitting. If you don't restart the kernel, there may be some code that you ran and deleted that is still being used and that was why your asserts were passing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "5c8ce6b8ae7f449c3fd4d44eeb3cf80b",
          "grade": false,
          "grade_id": "cell-0d4717fe7700aa92",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "iDFOcVgH_JXp"
      },
      "source": [
        "# Natural Language Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "ad29b177dd5a0376cd85ba9e39209fec",
          "grade": false,
          "grade_id": "cell-7ff8992190e52386",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "feIUZreM_JXq"
      },
      "source": [
        "# # Uncomment the below line to install\n",
        "! pip install spacy\n",
        "! python -m spacy download en_core_web_md"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "56a4068f8a3db8d2189569cde657aa44",
          "grade": false,
          "grade_id": "cell-839fb7e9db39b4dd",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "ieo4hUEZ_JXx"
      },
      "source": [
        "import sklearn\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, HashingVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
        "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
        "from sklearn.svm import LinearSVC\n",
        "import numpy as np\n",
        "import en_core_web_md"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "50df3022d50dc10fd18cd71acf03c2f5",
          "grade": false,
          "grade_id": "cell-32547bf8b3c17d81",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "gKP6IAM7_JX2",
        "outputId": "4defff65-c831-4910-f7e8-2e6061a1c7e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "data = fetch_20newsgroups(subset=\"all\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "1ee06fb20e81b0a6aeece342974b39bb",
          "grade": false,
          "grade_id": "cell-222fb692deae0d18",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "Hl4jhmSP_JX4"
      },
      "source": [
        "print(data.DESCR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "48fa44e3ed13130dd58386fbc41f1801",
          "grade": false,
          "grade_id": "cell-c1f44b15459bea17",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "T76RvRZ9_JX7",
        "outputId": "a3cc9a77-36d4-471c-d1a6-81dc01ebc9c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "text = data[\"data\"]\n",
        "target = data[\"target\"]\n",
        "print(\"The following are the 20 topics that an article can belong to:\")\n",
        "print(data[\"target_names\"])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The following are the 20 topics that an article can belong to:\n",
            "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "c2d0239fa14b9e082a02968d7c142595",
          "grade": false,
          "grade_id": "cell-3792303ddcf70dbb",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "trJS8uD9_JX9"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(text, target, random_state=0)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "ac68ffd4d73dff7c10eea2e716b9040a",
          "grade": false,
          "grade_id": "cell-2377d43a03f72b0f",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "pEYnRmvA_JX_",
        "outputId": "47ea1332-c1d9-4e0f-b953-47d373f941cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(f\"The training dataset contains {len(X_train)} articles.\")\n",
        "print(f\"The test dataset contains {len(X_test)} articles.\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The training dataset contains 14134 articles.\n",
            "The test dataset contains 4712 articles.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "d1991f8c56f04db0dad84fea9274d2f5",
          "grade": false,
          "grade_id": "cell-b2b050fc5a1f96db",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "QcO_5nn3_JYC"
      },
      "source": [
        "Scikit learn implements the BoW feature representation using [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html), and it also has implementations for [TF-IDF](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer) and [hashed vector](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html#sklearn.feature_extraction.text.HashingVectorizer) representations.\n",
        "\n",
        "Determine the feature representations of our dataset using each of those approaches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "45ef423d08f71eebb73f9e0f431566d2",
          "grade": false,
          "grade_id": "cell-f899f3942f01be93",
          "locked": false,
          "schema_version": 3,
          "solution": true
        },
        "id": "Wm3LDpu0_JYC",
        "outputId": "93e1ab47-d01d-4741-dc92-a8a7fe3cbd40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "# Use English stopwords and produce a BoW representation for the data using up to trigrams\n",
        "# Save the vectorizer as counter and the transformed data as X_train_bow, and X_test_bow\n",
        "counter = CountVectorizer(ngram_range=(1,3), stop_words='english')\n",
        "transformer = counter.fit(X_train)\n",
        "X_train_bow = transformer.transform(X_train)\n",
        "X_test_bow = transformer.transform(X_test)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 33.3 s, sys: 876 ms, total: 34.2 s\n",
            "Wall time: 34.2 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "54e331ac2a899a8d165562540b4e9c49",
          "grade": true,
          "grade_id": "cell-740433b073b13510",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false
        },
        "id": "xWWXHeWy_JYF"
      },
      "source": [
        "assert counter\n",
        "assert counter.stop_words == \"english\"\n",
        "assert counter.ngram_range == (1,3)\n",
        "assert len(counter.get_feature_names()) == 3034327\n",
        "assert X_train_bow.shape == (14134, 3034327)\n",
        "assert X_test_bow.shape == (4712, 3034327)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "630143b0c7f771b9c5855dbb5caf411d",
          "grade": false,
          "grade_id": "cell-cd1cb3b0c71eedd9",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "-wDgak3j_JYH"
      },
      "source": [
        "Note that sklearn implements a [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) and [TfidfTransformer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html). The main difference between the two is in the inputs to fitting and transforming. The [Vectorizer's fit/transform](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer.fit) take an input of text whereas the [transformer's](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer.fit) take an input of a BoW vector. Given that we already determined the BoW vectors, it would be more time efficient to use TfidfTransformer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "dee5b8fbe12f0499a87727cd864d345c",
          "grade": false,
          "grade_id": "cell-018bdbbae2613aef",
          "locked": false,
          "schema_version": 3,
          "solution": true
        },
        "id": "TMhnTa5o_JYH",
        "outputId": "3ecaf6d8-2e41-4b8e-8c61-ce40cdfde395",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "# Use the BoW representation you just created above to produce a TFIDF representation of the data\n",
        "# Save the transformer to tfidfer and the transformed data as X_train_tfidf, and X_test_tfidf\n",
        "\n",
        "tfidfer = TfidfTransformer().fit(X_train_bow)\n",
        "X_train_tfidf = tfidfer.transform(X_train_bow)\n",
        "X_test_tfidf = tfidfer.transform(X_test_bow)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2.75 s, sys: 14.9 ms, total: 2.76 s\n",
            "Wall time: 2.78 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "ecdb4782c18f141c55bfeefc8dec8c48",
          "grade": true,
          "grade_id": "cell-0e6ce5999cd93389",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false
        },
        "id": "RcVZowDA_JYK"
      },
      "source": [
        "assert tfidfer\n",
        "assert X_train_tfidf.shape  == (14134, 3034327)\n",
        "assert X_test_tfidf.shape  == (4712, 3034327)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "5f5c54e66ea2ef754e096d6523435c69",
          "grade": false,
          "grade_id": "cell-ed956dd2a828dea8",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "_hDsctz0_JYL"
      },
      "source": [
        "Now use the hashing vectorizer to do the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "2b1b4bcba6502dee96172ab281c7d4ca",
          "grade": false,
          "grade_id": "cell-2ad00d9b9df56ed5",
          "locked": false,
          "schema_version": 3,
          "solution": true
        },
        "id": "C2XzPmwc_JYM",
        "outputId": "334ad9fa-626f-47af-e993-97a10b34e716",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time \n",
        "# Use English stopwords and produce a Hashed vector representation for the data using up to trigrams\n",
        "# Save the vectorizer as hasher and the transformed data as X_train_hash, and X_test_hash\n",
        "# Make sure you set alternate_sign to False so we can use this representation with Multinomial Naive\n",
        "# Bayes later in the exercise.\n",
        "\n",
        "hasher = HashingVectorizer(ngram_range=(1,3), stop_words='english', alternate_sign=False)\n",
        "transformer = hasher.fit(X_train)\n",
        "X_train_hash = transformer.transform(X_train)\n",
        "X_test_hash = transformer.transform(X_test)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 6.73 s, sys: 1.81 ms, total: 6.73 s\n",
            "Wall time: 6.74 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "5fee5c79f5236e12894fdf52279d3f9a",
          "grade": true,
          "grade_id": "cell-f6a5afb5d4f0a4d9",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false
        },
        "id": "BHja1HBN_JYO"
      },
      "source": [
        "assert hasher\n",
        "assert hasher.stop_words == \"english\"\n",
        "assert hasher.ngram_range == (1,3)\n",
        "assert X_train_hash.shape == (14134, 1048576)\n",
        "assert X_test_hash.shape == (4712, 1048576)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "ca2e69878f74f9dfccd119da570bfe6d",
          "grade": false,
          "grade_id": "cell-f81192642c9b9950",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "JjrkFU3y_JYP"
      },
      "source": [
        "Compare the time it took to run the count vectorizer vs the hasing vectorizer even though they both will iterate through all the words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "e5d5d7774aecfc0c87da6a25ef08d25d",
          "grade": false,
          "grade_id": "cell-279d25a3bcd86cf6",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "NP0C7caG_JYQ"
      },
      "source": [
        "Now recall [Naive Bayes Classification](http://scikit-learn.org/stable/modules/naive_bayes.html) which we discussed early on in the supervised learning lectures. We will use Naive Bayes classifiers to predict the topic of the articles and compare our feature representations. Use a Multinomial Naive Bayes classifier to predict the topics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "e5257b0ce184bcaec3437d5f17b374a3",
          "grade": false,
          "grade_id": "cell-cdf5ba26c387fd97",
          "locked": false,
          "schema_version": 3,
          "solution": true
        },
        "id": "XTMI3-qK_JYQ",
        "outputId": "588d492a-d61e-4cfd-a0eb-9f7d85cf8a60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for feat_name, train_feat, test_feat in zip([\"Bag of Words\", \"TF-IDF\", \"Hashing\"],[X_train_bow, X_train_tfidf, X_train_hash], [X_test_bow, X_test_tfidf, X_test_hash]):\n",
        "    # Create a Multinomial Naive Bayes model saved to `mnb` and fit it to train_feat\n",
        "    mnb = MultinomialNB().fit(train_feat, y_train)\n",
        "    y_pred = mnb.predict(test_feat)\n",
        "    print(f\"Results for {feat_name}\")\n",
        "    print(\"-\"*80)\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(\"-\"*80)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results for Bag of Words\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.94      0.92       205\n",
            "           1       0.78      0.87      0.82       245\n",
            "           2       0.92      0.76      0.83       250\n",
            "           3       0.77      0.83      0.80       243\n",
            "           4       0.89      0.85      0.87       255\n",
            "           5       0.84      0.91      0.88       240\n",
            "           6       0.90      0.75      0.82       249\n",
            "           7       0.89      0.90      0.89       219\n",
            "           8       0.96      0.91      0.94       246\n",
            "           9       0.92      0.97      0.94       227\n",
            "          10       0.96      0.98      0.97       287\n",
            "          11       0.88      0.97      0.92       234\n",
            "          12       0.93      0.82      0.87       247\n",
            "          13       0.93      0.92      0.93       250\n",
            "          14       0.90      0.96      0.93       240\n",
            "          15       0.93      0.95      0.94       250\n",
            "          16       0.90      0.98      0.93       211\n",
            "          17       0.94      1.00      0.96       246\n",
            "          18       0.93      0.92      0.93       209\n",
            "          19       0.93      0.72      0.81       159\n",
            "\n",
            "    accuracy                           0.90      4712\n",
            "   macro avg       0.90      0.90      0.90      4712\n",
            "weighted avg       0.90      0.90      0.90      4712\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Results for TF-IDF\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.92      0.90       205\n",
            "           1       0.83      0.82      0.83       245\n",
            "           2       0.85      0.84      0.85       250\n",
            "           3       0.77      0.84      0.80       243\n",
            "           4       0.90      0.83      0.87       255\n",
            "           5       0.88      0.88      0.88       240\n",
            "           6       0.86      0.77      0.81       249\n",
            "           7       0.82      0.89      0.85       219\n",
            "           8       0.93      0.92      0.93       246\n",
            "           9       0.86      0.97      0.91       227\n",
            "          10       0.92      0.98      0.95       287\n",
            "          11       0.88      0.97      0.92       234\n",
            "          12       0.92      0.80      0.85       247\n",
            "          13       0.96      0.88      0.92       250\n",
            "          14       0.87      0.96      0.91       240\n",
            "          15       0.78      0.94      0.85       250\n",
            "          16       0.83      0.98      0.90       211\n",
            "          17       0.95      0.99      0.97       246\n",
            "          18       0.99      0.77      0.87       209\n",
            "          19       0.92      0.42      0.57       159\n",
            "\n",
            "    accuracy                           0.88      4712\n",
            "   macro avg       0.88      0.87      0.87      4712\n",
            "weighted avg       0.88      0.88      0.87      4712\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Results for Hashing\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.87      0.87       205\n",
            "           1       0.78      0.80      0.79       245\n",
            "           2       0.84      0.80      0.82       250\n",
            "           3       0.71      0.82      0.76       243\n",
            "           4       0.91      0.80      0.85       255\n",
            "           5       0.88      0.86      0.87       240\n",
            "           6       0.85      0.78      0.81       249\n",
            "           7       0.82      0.89      0.85       219\n",
            "           8       0.89      0.91      0.90       246\n",
            "           9       0.82      0.96      0.88       227\n",
            "          10       0.93      0.95      0.94       287\n",
            "          11       0.83      0.97      0.90       234\n",
            "          12       0.90      0.77      0.83       247\n",
            "          13       0.95      0.86      0.91       250\n",
            "          14       0.86      0.97      0.91       240\n",
            "          15       0.75      0.94      0.83       250\n",
            "          16       0.79      0.97      0.87       211\n",
            "          17       0.93      0.98      0.95       246\n",
            "          18       0.99      0.69      0.81       209\n",
            "          19       0.98      0.27      0.42       159\n",
            "\n",
            "    accuracy                           0.85      4712\n",
            "   macro avg       0.86      0.84      0.84      4712\n",
            "weighted avg       0.86      0.85      0.85      4712\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "c6868948856264f42927eef81e550203",
          "grade": true,
          "grade_id": "cell-b545c8d3793a05e7",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false
        },
        "id": "0rcGydZO_JYS"
      },
      "source": [
        "assert isinstance(mnb, MultinomialNB)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "0e12e32a8bcd208a60db88b25a2f2ab2",
          "grade": false,
          "grade_id": "cell-f1fb206ff47bb7a6",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "DM0C227F_JYU"
      },
      "source": [
        "## Learned Embeddings\n",
        "\n",
        "We will use [spacy](https://spacy.io/) for more sophisticated NLP. Make sure you downloaded the english model in the commented code at the top of the notebook before proceeding. It may take some time to download.\n",
        "\n",
        "Spacy allows us to parse text and automatically does the following:\n",
        "- tokenization\n",
        "- lemmatization\n",
        "- sentence splitting\n",
        "- entity recognition\n",
        "- token vector representation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "2c063a433e96171b19c013141d9aed53",
          "grade": false,
          "grade_id": "cell-bdd9ef5d3e965340",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "Cl47CyDH_JYU",
        "outputId": "e6aebe3f-8b1f-459c-dd1e-21bf2a071599",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "nlp = en_core_web_md.load()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 17.6 s, sys: 332 ms, total: 18 s\n",
            "Wall time: 18 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "dde683a4f8f25f1cf4e3cb04bd092989",
          "grade": false,
          "grade_id": "cell-11e9ffbf2f50c6eb",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "y0wPJ3ao_JYW"
      },
      "source": [
        "text = \"This is the first sentence in this test string. The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "parsed_text = nlp(text)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "11bf7045ed3934b887d3f3c1edd270bf",
          "grade": false,
          "grade_id": "cell-d4194dd187aa95b1",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "P5p_JV4Q_JYY",
        "outputId": "27e9b770-fced-440f-e705-c26e2c939833",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for sent in parsed_text.sents:\n",
        "    print(f\"Analyzing sentence: {sent}\")\n",
        "    print(f\"Lemmatization: {sent.lemma_}\")\n",
        "    for token in sent:\n",
        "        print(f\"Analyzing token: {token}\")\n",
        "        if token.is_sent_start:\n",
        "            print(\"This token is the first one in the sentence\")\n",
        "        if token.is_stop:\n",
        "            print(\"Stop word\")\n",
        "        else:\n",
        "            print(\"Not stop word\")\n",
        "        print(f\"Entity type: {token.ent_type_}\")\n",
        "        print(f\"Part of speech: {token.pos_}\")\n",
        "        print(f\"Lemma: {token.lemma_}\")\n",
        "        print(\"-\"*10)\n",
        "    print(\"-\"*50)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Analyzing sentence: This is the first sentence in this test string.\n",
            "Lemmatization: this be the first sentence in this test string .\n",
            "Analyzing token: This\n",
            "This token is the first one in the sentence\n",
            "Stop word\n",
            "Entity type: \n",
            "Part of speech: DET\n",
            "Lemma: this\n",
            "----------\n",
            "Analyzing token: is\n",
            "Stop word\n",
            "Entity type: \n",
            "Part of speech: AUX\n",
            "Lemma: be\n",
            "----------\n",
            "Analyzing token: the\n",
            "Stop word\n",
            "Entity type: \n",
            "Part of speech: DET\n",
            "Lemma: the\n",
            "----------\n",
            "Analyzing token: first\n",
            "Stop word\n",
            "Entity type: ORDINAL\n",
            "Part of speech: ADJ\n",
            "Lemma: first\n",
            "----------\n",
            "Analyzing token: sentence\n",
            "Not stop word\n",
            "Entity type: \n",
            "Part of speech: NOUN\n",
            "Lemma: sentence\n",
            "----------\n",
            "Analyzing token: in\n",
            "Stop word\n",
            "Entity type: \n",
            "Part of speech: ADP\n",
            "Lemma: in\n",
            "----------\n",
            "Analyzing token: this\n",
            "Stop word\n",
            "Entity type: \n",
            "Part of speech: DET\n",
            "Lemma: this\n",
            "----------\n",
            "Analyzing token: test\n",
            "Not stop word\n",
            "Entity type: \n",
            "Part of speech: NOUN\n",
            "Lemma: test\n",
            "----------\n",
            "Analyzing token: string\n",
            "Not stop word\n",
            "Entity type: \n",
            "Part of speech: NOUN\n",
            "Lemma: string\n",
            "----------\n",
            "Analyzing token: .\n",
            "Not stop word\n",
            "Entity type: \n",
            "Part of speech: PUNCT\n",
            "Lemma: .\n",
            "----------\n",
            "--------------------------------------------------\n",
            "Analyzing sentence: The quick brown fox jumps over the lazy dog.\n",
            "Lemmatization: the quick brown fox jump over the lazy dog .\n",
            "Analyzing token: The\n",
            "This token is the first one in the sentence\n",
            "Stop word\n",
            "Entity type: \n",
            "Part of speech: DET\n",
            "Lemma: the\n",
            "----------\n",
            "Analyzing token: quick\n",
            "Not stop word\n",
            "Entity type: \n",
            "Part of speech: ADJ\n",
            "Lemma: quick\n",
            "----------\n",
            "Analyzing token: brown\n",
            "Not stop word\n",
            "Entity type: PERSON\n",
            "Part of speech: ADJ\n",
            "Lemma: brown\n",
            "----------\n",
            "Analyzing token: fox\n",
            "Not stop word\n",
            "Entity type: PERSON\n",
            "Part of speech: NOUN\n",
            "Lemma: fox\n",
            "----------\n",
            "Analyzing token: jumps\n",
            "Not stop word\n",
            "Entity type: \n",
            "Part of speech: VERB\n",
            "Lemma: jump\n",
            "----------\n",
            "Analyzing token: over\n",
            "Stop word\n",
            "Entity type: \n",
            "Part of speech: ADP\n",
            "Lemma: over\n",
            "----------\n",
            "Analyzing token: the\n",
            "Stop word\n",
            "Entity type: \n",
            "Part of speech: DET\n",
            "Lemma: the\n",
            "----------\n",
            "Analyzing token: lazy\n",
            "Not stop word\n",
            "Entity type: \n",
            "Part of speech: ADJ\n",
            "Lemma: lazy\n",
            "----------\n",
            "Analyzing token: dog\n",
            "Not stop word\n",
            "Entity type: \n",
            "Part of speech: NOUN\n",
            "Lemma: dog\n",
            "----------\n",
            "Analyzing token: .\n",
            "Not stop word\n",
            "Entity type: \n",
            "Part of speech: PUNCT\n",
            "Lemma: .\n",
            "----------\n",
            "--------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "778289b88c9003cc27f7839543106eb6",
          "grade": false,
          "grade_id": "cell-a1f85ff1f2b6b116",
          "locked": false,
          "schema_version": 3,
          "solution": true
        },
        "id": "E0PSR54U_JYZ"
      },
      "source": [
        "### Come up with a couple sentences to test out and set the text to my_text\n",
        "### You can go to your favorite website or news source and copy a paragraph from there\n",
        "\n",
        "my_text = \"\"\"We may sometimes have hidden tests for grading. This means that passing the visible assert statements is not sufficient. \n",
        "The assert statements are there as a guide but you need to make sure you understand what you're required to do and ensure that you are doing it correctly. \n",
        "Passing the visible tests is necessary but not sufficient to get the grade for that cell.\"\"\""
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "aadee940b59935acd80563ce3c6b248e",
          "grade": true,
          "grade_id": "cell-dd4c284f3101e2e6",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false
        },
        "id": "7l13waJt_JYc"
      },
      "source": [
        "assert len(my_text) > 10\n",
        "assert my_text.count(\".\") > 2"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "f6193aa815fad57f5db9423bdb6d8b34",
          "grade": false,
          "grade_id": "cell-fd7fed9c7e3903f4",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "l61r370j_JYe",
        "outputId": "f4388e3e-7d48-42fc-d659-a01ae82a88e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "parsed = nlp(my_text)\n",
        "for sent in parsed.sents:\n",
        "    print(f\"Analyzing sentence: {sent}\")\n",
        "    print(f\"Lemmatization: {sent.lemma_}\")\n",
        "    for token in sent:\n",
        "        print(f\"Analyzing token: {token}\")\n",
        "        if token.is_sent_start:\n",
        "            print(\"This token is the first one in the sentence\")\n",
        "        if token.is_stop:\n",
        "            print(\"Stop word\")\n",
        "        else:\n",
        "            print(\"Not stop word\")\n",
        "        print(f\"Entity type: {token.ent_type_}\")\n",
        "        print(f\"Part of speech: {token.pos_}\")\n",
        "        print(f\"Lemma: {token.lemma_}\")\n",
        "        print(\"-\"*10)\n",
        "    print(\"-\"*50)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Analyzing sentence: We may sometimes have hidden tests for grading.\n",
            "Lemmatization: -PRON- may sometimes have hide test for grading .\n",
            "Analyzing token: We\n",
            "This token is the first one in the sentence\n",
            "Stop word\n",
            "Entity type: \n",
            "Part of speech: PRON\n",
            "Lemma: -PRON-\n",
            "----------\n",
            "Analyzing token: may\n",
            "Stop word\n",
            "Entity type: \n",
            "Part of speech: VERB\n",
            "Lemma: may\n",
            "----------\n",
            "Analyzing token: sometimes\n",
            "Stop word\n",
            "Entity type: \n",
            "Part of speech: ADV\n",
            "Lemma: sometimes\n",
            "----------\n",
            "Analyzing token: have\n",
            "Stop word\n",
            "Entity type: \n",
            "Part of speech: AUX\n",
            "Lemma: have\n",
            "----------\n",
            "Analyzing token: hidden\n",
            "Not stop word\n",
            "Entity type: \n",
            "Part of speech: VERB\n",
            "Lemma: hide\n",
            "----------\n",
            "Analyzing token: tests\n",
            "Not stop word\n",
            "Entity type: \n",
            "Part of speech: NOUN\n",
            "Lemma: test\n",
            "----------\n",
            "Analyzing token: for\n",
            "Stop word\n",
            "Entity type: \n",
            "Part of speech: ADP\n",
            "Lemma: for\n",
            "----------\n",
            "Analyzing token: grading\n",
            "Not stop word\n",
            "Entity type: \n",
            "Part of speech: NOUN\n",
            "Lemma: grading\n",
            "----------\n",
            "Analyzing token: .\n",
            "Not stop word\n",
            "Entity type: \n",
            "Part of speech: PUNCT\n",
            "Lemma: .\n",
            "----------\n",
            "--------------------------------------------------\n",
            "Analyzing sentence: This means that passing the visible assert statements is not sufficient. \n",
            "\n",
            "Lemmatization: this mean that pass the visible assert statement be not sufficient .\n",
            "Analyzing token: This\n",
            "This token is the first one in the sentence\n",
            "Stop word\n",
            "Entity type: \n",
            "Part of speech: DET\n",
            "Lemma: this\n",
            "----------\n",
            "Analyzing token: means\n",
            "Not stop word\n",
            "Entity type: \n",
            "Part of speech: VERB\n",
            "Lemma: mean\n",
            "----------\n",
            "Analyzing token: that\n",
            "Stop word\n",
            "Entity type: \n",
            "Part of speech: SCONJ\n",
            "Lemma: that\n",
            "----------\n",
            "Analyzing token: passing\n",
            "Not stop word\n",
            "Entity type: \n",
            "Part of speech: VERB\n",
            "Lemma: pass\n",
            "----------\n",
            "Analyzing token: the\n",
            "Stop word\n",
            "Entity type: \n",
            "Part of speech: DET\n",
            "Lemma: the\n",
            "----------\n",
            "Analyzing token: visible\n",
            "Not stop word\n",
            "Entity type: \n",
            "Part of speech: ADJ\n",
            "Lemma: visible\n",
            "----------\n",
            "Analyzing token: assert\n",
            "Not stop word\n",
            "Entity type: \n",
            "Part of speech: NOUN\n",
            "Lemma: assert\n",
            "----------\n",
            "Analyzing token: statements\n",
            "Not stop word\n",
            "Entity type: \n",
            "Part of speech: NOUN\n",
            "Lemma: statement\n",
            "----------\n",
            "Analyzing token: is\n",
            "Stop word\n",
            "Entity type: \n",
            "Part of speech: AUX\n",
            "Lemma: be\n",
            "----------\n",
            "Analyzing token: not\n",
            "Stop word\n",
            "Entity type: \n",
            "Part of speech: PART\n",
            "Lemma: not\n",
            "----------\n",
            "Analyzing token: sufficient\n",
            "Not stop word\n",
            "Entity type: \n",
            "Part of speech: ADJ\n",
            "Lemma: sufficient\n",
            "----------\n",
            "Analyzing token: .\n",
            "Not stop word\n",
            "Entity type: \n",
            "Part of speech: PUNCT\n",
            "Lemma: .\n",
            "----------\n",
            "Analyzing token: \n",
            "\n",
            "Not stop word\n",
            "Entity type: \n",
            "Part of speech: SPACE\n",
            "Lemma: \n",
            "\n",
            "----------\n",
            "--------------------------------------------------\n",
            "Analyzing sentence: The assert statements are there as a guide\n",
            "Lemmatization: the assert statement be there as a guide\n",
            "Analyzing token: The\n",
            "This token is the first one in the sentence\n",
            "Stop word\n",
            "Entity type: \n",
            "Part of speech: DET\n",
            "Lemma: the\n",
            "----------\n",
            "Analyzing token: assert\n",
            "Not stop word\n",
            "Entity type: \n",
            "Part of speech: NOUN\n",
            "Lemma: assert\n",
            "----------\n",
            "Analyzing token: statements\n",
            "Not stop word\n",
            "Entity type: \n",
            "Part of speech: NOUN\n",
            "Lemma: statement\n",
            "----------\n",
            "Analyzing token: are\n",
            "Stop word\n",
            "Entity type: \n",
            "Part of speech: AUX\n",
            "Lemma: be\n",
            "----------\n",
            "Analyzing token: there\n",
            "Stop word\n",
            "Entity type: \n",
            "Part of speech: ADV\n",
            "Lemma: there\n",
            "----------\n",
            "Analyzing token: as\n",
            "Stop word\n",
            "Entity type: \n",
            "Part of speech: SCONJ\n",
            "Lemma: as\n",
            "----------\n",
            "Analyzing token: a\n",
            "Stop word\n",
            "Entity type: \n",
            "Part of speech: DET\n",
            "Lemma: a\n",
            "----------\n",
            "Analyzing token: guide\n",
            "Not stop word\n",
            "Entity type: \n",
            "Part of speech: NOUN\n",
            "Lemma: guide\n",
            "----------\n",
            "--------------------------------------------------\n",
            "Analyzing sentence: but you need to make sure you understand what you're required to do and ensure that you are doing it correctly. \n",
            "\n",
            "Lemmatization: but -PRON- need to make sure -PRON- understand what -PRON- be require to do and ensure that -PRON- be do -PRON- correctly .\n",
            "Analyzing token: but\n",
            "This token is the first one in the sentence\n",
            "Stop word\n",
            "Entity type: \n",
            "Part of speech: CCONJ\n",
            "Lemma: but\n",
            "----------\n",
            "Analyzing token: you\n",
            "Stop word\n",
            "Entity type: \n",
            "Part of speech: PRON\n",
            "Lemma: -PRON-\n",
            "----------\n",
            "Analyzing token: need\n",
            "Not stop word\n",
            "Entity type: \n",
            "Part of speech: VERB\n",
            "Lemma: need\n",
            "----------\n",
            "Analyzing token: to\n",
            "Stop word\n",
            "Entity type: \n",
            "Part of speech: PART\n",
            "Lemma: to\n",
            "----------\n",
            "Analyzing token: make\n",
            "Stop word\n",
            "Entity type: \n",
            "Part of speech: VERB\n",
            "Lemma: make\n",
            "----------\n",
            "Analyzing token: sure\n",
            "Not stop word\n",
            "Entity type: \n",
            "Part of speech: ADJ\n",
            "Lemma: sure\n",
            "----------\n",
            "Analyzing token: you\n",
            "Stop word\n",
            "Entity type: \n",
            "Part of speech: PRON\n",
            "Lemma: -PRON-\n",
            "----------\n",
            "Analyzing token: understand\n",
            "Not stop word\n",
            "Entity type: \n",
            "Part of speech: VERB\n",
            "Lemma: understand\n",
            "----------\n",
            "Analyzing token: what\n",
            "Stop word\n",
            "Entity type: \n",
            "Part of speech: PRON\n",
            "Lemma: what\n",
            "----------\n",
            "Analyzing token: you\n",
            "Stop word\n",
            "Entity type: \n",
            "Part of speech: PRON\n",
            "Lemma: -PRON-\n",
            "----------\n",
            "Analyzing token: 're\n",
            "Stop word\n",
            "Entity type: \n",
            "Part of speech: AUX\n",
            "Lemma: be\n",
            "----------\n",
            "Analyzing token: required\n",
            "Not stop word\n",
            "Entity type: \n",
            "Part of speech: VERB\n",
            "Lemma: require\n",
            "----------\n",
            "Analyzing token: to\n",
            "Stop word\n",
            "Entity type: \n",
            "Part of speech: PART\n",
            "Lemma: to\n",
            "----------\n",
            "Analyzing token: do\n",
            "Stop word\n",
            "Entity type: \n",
            "Part of speech: AUX\n",
            "Lemma: do\n",
            "----------\n",
            "Analyzing token: and\n",
            "Stop word\n",
            "Entity type: \n",
            "Part of speech: CCONJ\n",
            "Lemma: and\n",
            "----------\n",
            "Analyzing token: ensure\n",
            "Not stop word\n",
            "Entity type: \n",
            "Part of speech: VERB\n",
            "Lemma: ensure\n",
            "----------\n",
            "Analyzing token: that\n",
            "Stop word\n",
            "Entity type: \n",
            "Part of speech: SCONJ\n",
            "Lemma: that\n",
            "----------\n",
            "Analyzing token: you\n",
            "Stop word\n",
            "Entity type: \n",
            "Part of speech: PRON\n",
            "Lemma: -PRON-\n",
            "----------\n",
            "Analyzing token: are\n",
            "Stop word\n",
            "Entity type: \n",
            "Part of speech: AUX\n",
            "Lemma: be\n",
            "----------\n",
            "Analyzing token: doing\n",
            "Stop word\n",
            "Entity type: \n",
            "Part of speech: VERB\n",
            "Lemma: do\n",
            "----------\n",
            "Analyzing token: it\n",
            "Stop word\n",
            "Entity type: \n",
            "Part of speech: PRON\n",
            "Lemma: -PRON-\n",
            "----------\n",
            "Analyzing token: correctly\n",
            "Not stop word\n",
            "Entity type: \n",
            "Part of speech: ADV\n",
            "Lemma: correctly\n",
            "----------\n",
            "Analyzing token: .\n",
            "Not stop word\n",
            "Entity type: \n",
            "Part of speech: PUNCT\n",
            "Lemma: .\n",
            "----------\n",
            "Analyzing token: \n",
            "\n",
            "Not stop word\n",
            "Entity type: \n",
            "Part of speech: SPACE\n",
            "Lemma: \n",
            "\n",
            "----------\n",
            "--------------------------------------------------\n",
            "Analyzing sentence: Passing the visible tests is necessary but not sufficient to get the grade for that cell.\n",
            "Lemmatization: pass the visible test be necessary but not sufficient to get the grade for that cell .\n",
            "Analyzing token: Passing\n",
            "This token is the first one in the sentence\n",
            "Not stop word\n",
            "Entity type: \n",
            "Part of speech: VERB\n",
            "Lemma: pass\n",
            "----------\n",
            "Analyzing token: the\n",
            "Stop word\n",
            "Entity type: \n",
            "Part of speech: DET\n",
            "Lemma: the\n",
            "----------\n",
            "Analyzing token: visible\n",
            "Not stop word\n",
            "Entity type: \n",
            "Part of speech: ADJ\n",
            "Lemma: visible\n",
            "----------\n",
            "Analyzing token: tests\n",
            "Not stop word\n",
            "Entity type: \n",
            "Part of speech: NOUN\n",
            "Lemma: test\n",
            "----------\n",
            "Analyzing token: is\n",
            "Stop word\n",
            "Entity type: \n",
            "Part of speech: AUX\n",
            "Lemma: be\n",
            "----------\n",
            "Analyzing token: necessary\n",
            "Not stop word\n",
            "Entity type: \n",
            "Part of speech: ADJ\n",
            "Lemma: necessary\n",
            "----------\n",
            "Analyzing token: but\n",
            "Stop word\n",
            "Entity type: \n",
            "Part of speech: CCONJ\n",
            "Lemma: but\n",
            "----------\n",
            "Analyzing token: not\n",
            "Stop word\n",
            "Entity type: \n",
            "Part of speech: PART\n",
            "Lemma: not\n",
            "----------\n",
            "Analyzing token: sufficient\n",
            "Not stop word\n",
            "Entity type: \n",
            "Part of speech: ADJ\n",
            "Lemma: sufficient\n",
            "----------\n",
            "Analyzing token: to\n",
            "Stop word\n",
            "Entity type: \n",
            "Part of speech: PART\n",
            "Lemma: to\n",
            "----------\n",
            "Analyzing token: get\n",
            "Stop word\n",
            "Entity type: \n",
            "Part of speech: AUX\n",
            "Lemma: get\n",
            "----------\n",
            "Analyzing token: the\n",
            "Stop word\n",
            "Entity type: \n",
            "Part of speech: DET\n",
            "Lemma: the\n",
            "----------\n",
            "Analyzing token: grade\n",
            "Not stop word\n",
            "Entity type: \n",
            "Part of speech: NOUN\n",
            "Lemma: grade\n",
            "----------\n",
            "Analyzing token: for\n",
            "Stop word\n",
            "Entity type: \n",
            "Part of speech: ADP\n",
            "Lemma: for\n",
            "----------\n",
            "Analyzing token: that\n",
            "Stop word\n",
            "Entity type: \n",
            "Part of speech: DET\n",
            "Lemma: that\n",
            "----------\n",
            "Analyzing token: cell\n",
            "Not stop word\n",
            "Entity type: \n",
            "Part of speech: NOUN\n",
            "Lemma: cell\n",
            "----------\n",
            "Analyzing token: .\n",
            "Not stop word\n",
            "Entity type: \n",
            "Part of speech: PUNCT\n",
            "Lemma: .\n",
            "----------\n",
            "--------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "a200a636496b7c4b0a02f61e4ac84a2f",
          "grade": false,
          "grade_id": "cell-1a2a413b9c440b95",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "9SmtkBdG_JYf"
      },
      "source": [
        "If we use the larger spacy models, we get the GloVe representation for some words based on a pre-trained model. The GloVe vectors should be in 300 dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "94d3c1d517f9151fa1f3b6de01983a48",
          "grade": false,
          "grade_id": "cell-da272ac031f8d791",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "1Nou2CSF_JYg",
        "outputId": "f0664c93-ec99-4b4c-989f-aebe30c4b09a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "token.vector.shape"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "3c68c644df1851056a4f8f93cda99494",
          "grade": false,
          "grade_id": "cell-7d1247a2cf60bab3",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "CoX03-gr_JYh"
      },
      "source": [
        "Given that the parsing of text takes some time, we will only consider the first 1000 articles in our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "2d075c5070bd31c803ad35d1e57b88ee",
          "grade": false,
          "grade_id": "cell-65e1d1dbb5567b87",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "WI0CwWGs_JYi"
      },
      "source": [
        "new_X_train, new_X_test, new_y_train, new_y_test = train_test_split(X_train[:1000], y_train[:1000], random_state=0)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "8daa6c62df8c89646e4aeda76b252ac0",
          "grade": false,
          "grade_id": "cell-a35e8a4ea5c845f6",
          "locked": false,
          "schema_version": 3,
          "solution": true
        },
        "id": "J1-iAbUm_JYk",
        "outputId": "8e3cba3f-4128-4ed8-9ad3-7a45daaeca14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "# Using nlp from above, parse every instance of new_X_train\n",
        "# save the document vectors to a np.array called X_train_glove\n",
        "\n",
        "X_train_glove = np.empty((len(new_X_train), token.vector.shape[0]))\n",
        "X_test_glove = np.empty((len(new_X_test), token.vector.shape[0]))\n",
        "\n",
        "for i in range(len(new_X_train)):\n",
        "  temp_text = new_X_train[i]\n",
        "  parsed = nlp(temp_text)\n",
        "  X_train_glove[i,:] = parsed.vector\n",
        "for i in range(len(new_X_test)):\n",
        "  temp_text = new_X_test[i]\n",
        "  parsed = nlp(temp_text)\n",
        "  X_test_glove[i,:] = parsed.vector"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 7s, sys: 2.76 s, total: 1min 10s\n",
            "Wall time: 1min 10s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "3cbbe22214eea789813b27bee43dc314",
          "grade": true,
          "grade_id": "cell-8431bb15ebe0914f",
          "locked": true,
          "points": 3,
          "schema_version": 3,
          "solution": false
        },
        "id": "bfi3xIM4_JYm"
      },
      "source": [
        "assert X_train_glove.shape == (len(new_X_train), 300)\n",
        "assert X_test_glove.shape == (len(new_X_test), 300)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "6d13cff29a75bb3f0b8087af24238c43",
          "grade": false,
          "grade_id": "cell-5b693b9255ee6aea",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "kvXqM2Ir_JYn",
        "outputId": "2df8f9ba-1bee-4b40-d4cc-2a73cc5eb18e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "svm = LinearSVC().fit(X_train_glove, new_y_train)\n",
        "y_pred = svm.predict(X_test_glove)\n",
        "print(classification_report(new_y_test, y_pred))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         7\n",
            "           1       0.50      0.50      0.50        10\n",
            "           2       0.60      0.55      0.57        11\n",
            "           3       0.60      0.50      0.55        12\n",
            "           4       0.50      0.50      0.50        12\n",
            "           5       0.58      0.64      0.61        11\n",
            "           6       0.64      0.90      0.75        10\n",
            "           7       0.77      0.94      0.85        18\n",
            "           8       0.88      0.82      0.85        17\n",
            "           9       0.76      0.76      0.76        17\n",
            "          10       0.64      0.64      0.64        14\n",
            "          11       0.85      0.79      0.81        14\n",
            "          12       0.69      0.47      0.56        19\n",
            "          13       0.80      1.00      0.89        16\n",
            "          14       0.75      0.60      0.67        10\n",
            "          15       0.56      0.77      0.65        13\n",
            "          16       0.92      0.79      0.85        14\n",
            "          17       0.73      0.89      0.80         9\n",
            "          18       0.80      0.73      0.76        11\n",
            "          19       0.20      0.20      0.20         5\n",
            "\n",
            "    accuracy                           0.69       250\n",
            "   macro avg       0.64      0.65      0.64       250\n",
            "weighted avg       0.68      0.69      0.68       250\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "7e662876feeb742e62383826b6fcb418",
          "grade": false,
          "grade_id": "cell-cbdc66865e98beb5",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "HgEJepp2_JYp"
      },
      "source": [
        "We will not cover LDA in this exercise but if you are interested in topic modeling, you should check out [Gensim](https://radimrehurek.com/gensim/) and its [LDA implementation](https://radimrehurek.com/gensim/models/ldamodel.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "483e65acc4062f57f7320d9b4cb0f945",
          "grade": false,
          "grade_id": "cell-75181722913aa756",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "ixbeFaBt_JYq"
      },
      "source": [
        "## Feedback"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "ed936ab53a1391c5e6af8df699a1dbf5",
          "grade": false,
          "grade_id": "feedback",
          "locked": false,
          "schema_version": 3,
          "solution": true
        },
        "id": "Boi7ZgnN_JYq"
      },
      "source": [
        "def feedback():\n",
        "    \"\"\"Provide feedback on the contents of this exercise\n",
        "    \n",
        "    Returns:\n",
        "        string\n",
        "    \"\"\"\n",
        "    return \"All good!\""
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "f39f6185a54850c2f1f9b5b2a17b7543",
          "grade": true,
          "grade_id": "feedback-tests",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false
        },
        "id": "vpWmOBf4_JYr",
        "outputId": "9d3433e0-1343-4488-99cc-6ac6e37e2fe7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(feedback())"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All good!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}